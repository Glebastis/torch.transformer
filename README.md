# torch.transformer
My first step in transformer legacy =) 

This verson is constructed from other sources of transformer implementation./n
Yet it has no masking at Decoder, so you should be aware of that./n
This implementation has multi-head self-attention and positional encoding /n
which gives the nn the structure of sequence.
