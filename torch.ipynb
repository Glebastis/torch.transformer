{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d51de4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from psutil import virtual_memory\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import time\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "import torchvision.transforms as transforms\n",
    "from torchsummary import summary #pip install torchsummary\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "0310a136",
   "metadata": {},
   "outputs": [],
   "source": [
    "ram_bit = virtual_memory().total\n",
    "ram_gb = ram_bit / (2**10) / (2**10) / (2**10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "a4e6f284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31.300193786621094"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ram_gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a486946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdown.download('https://storage.yandexcloud.net/aiueducation/Content/advanced/l3/rus-eng.zip', None, quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60c42f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -o rus-eng.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d951c378",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "with open(\"rus.txt\", 'r', encoding='utf-8') as f: # Открываем файл словаря в режиме чтения\n",
    "    lines = f.read().split('\\n')                  # Читаем весь файл, режем на строки\n",
    "\n",
    "for i,line in enumerate(lines):\n",
    "    try:\n",
    "        if len(line.split(\"\\t\")[0])<=40:\n",
    "            input_text, target_text, _ = line.split(\"\\t\")\n",
    "            \n",
    "            train.append({'rus':target_text, 'eng':input_text})\n",
    "    except:\n",
    "        print(line == '')\n",
    "        continue\n",
    "\n",
    "# ds = tf.data.Dataset.from_tensor_slices(pd.DataFrame.from_dict(train).to_dict(orient=\"list\"))\n",
    "\n",
    "tokenizer_ru = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "    (pd.DataFrame.from_dict(train).to_dict(orient=\"list\")['rus']), target_vocab_size=2**13)\n",
    "\n",
    "tokenizer_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "    (pd.DataFrame.from_dict(train).to_dict(orient=\"list\")['eng']), target_vocab_size=2**13)\n",
    "\n",
    "\n",
    "both = np.array(list(map(lambda x:[x['rus'],x['eng']], train)))\n",
    "for_filter = np.array(list(map(lambda x: True if '\\xa0' not in x else False, both[:,1])))\n",
    "both = both[for_filter]\n",
    "\n",
    "ru_corpus = np.array(list(map(lambda x:torch.Tensor(np.array([tokenizer_ru.vocab_size] + tokenizer_ru.encode(x) + \\\n",
    "                     [tokenizer_ru.vocab_size+1])).type(torch.int64), both[:,0])))\n",
    "\n",
    "en_corpus = np.array(list(map(lambda x:torch.Tensor(np.array([tokenizer_en.vocab_size] + \\\n",
    "                                                             tokenizer_en.encode(x))).type(torch.int64), both[:,1])))\n",
    "\n",
    "en_target = np.array(list(map(lambda x:torch.Tensor(np.array(tokenizer_en.encode(x) + \\\n",
    "                                     [tokenizer_en.vocab_size+1])).type(torch.int64), both[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ecd2ba3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/tunz/transformer-pytorch/blob/master/model/transformer.py\n",
    "\n",
    "def initialize_weight(x):\n",
    "    nn.init.xavier_uniform_(x.weight)\n",
    "    if x.bias is not None:\n",
    "        nn.init.constant_(x.bias, 0)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, dropout_rate, head_size=8):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.head_size = head_size\n",
    "\n",
    "        self.att_size = att_size = hidden_size // head_size\n",
    "        self.scale = att_size ** -0.5\n",
    "\n",
    "        self.linear_q = nn.Linear(hidden_size, head_size * att_size, bias=False)\n",
    "        self.linear_k = nn.Linear(hidden_size, head_size * att_size, bias=False)\n",
    "        self.linear_v = nn.Linear(hidden_size, head_size * att_size, bias=False)\n",
    "        \n",
    "        initialize_weight(self.linear_q)\n",
    "        initialize_weight(self.linear_k)\n",
    "        initialize_weight(self.linear_v)\n",
    "\n",
    "        self.att_dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.output_layer = nn.Linear(head_size * att_size, hidden_size,\n",
    "                                      bias=False)\n",
    "        initialize_weight(self.output_layer)\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "        orig_q_size = q.size()\n",
    "\n",
    "        d_k = self.att_size\n",
    "        d_v = self.att_size\n",
    "        batch_size = q.size(0)\n",
    "\n",
    "        # head_i = Attention(Q(W^Q)_i, K(W^K)_i, V(W^V)_i)\n",
    "        q = self.linear_q(q).view(batch_size, -1, self.head_size, d_k)\n",
    "        k = self.linear_k(k).view(batch_size, -1, self.head_size, d_k)\n",
    "        v = self.linear_v(v).view(batch_size, -1, self.head_size, d_v)\n",
    "\n",
    "        q = q.transpose(1, 2)                  # [b, h, q_len, d_k]\n",
    "        v = v.transpose(1, 2)                  # [b, h, v_len, d_v]\n",
    "        k = k.transpose(1, 2).transpose(2, 3)  # [b, h, d_k, k_len]\n",
    "\n",
    "        # Scaled Dot-Product Attention.\n",
    "        # Attention(Q, K, V) = softmax((QK^T)/sqrt(d_k))V\n",
    "        q.mul_(self.scale)\n",
    "        x = torch.matmul(q, k)  # [b, h, q_len, k_len]\n",
    "\n",
    "        x = torch.softmax(x, dim=3)\n",
    "        x = self.att_dropout(x)\n",
    "        x = x.matmul(v)  # [b, h, q_len, attn]\n",
    "\n",
    "        x = x.transpose(1, 2).contiguous()  # [b, q_len, h, attn]\n",
    "        x = x.view(batch_size, -1, self.head_size * d_v)\n",
    "\n",
    "        x = self.output_layer(x)\n",
    "\n",
    "        assert x.size() == orig_q_size\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c1c41cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_size, filter_size, dropout_rate):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        \n",
    "        self.self_attention_norm = nn.LayerNorm(hidden_size, eps=1e-6)\n",
    "        self.self_attention = MultiHeadAttention(hidden_size, dropout_rate)\n",
    "        self.self_attention_dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.ffn_norm = nn.LayerNorm(hidden_size, eps=1e-6)\n",
    "        self.ffn = FeedForwardNetwork(hidden_size, filter_size, dropout_rate)\n",
    "        self.ffn_dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):  # pylint: disable=arguments-differ\n",
    "        y = self.self_attention_norm(x)\n",
    "        y = self.self_attention(y, y, y)\n",
    "        y = self.self_attention_dropout(y)\n",
    "        x = x + y\n",
    "\n",
    "        y = self.ffn_norm(x)\n",
    "        y = self.ffn(y)\n",
    "        y = self.ffn_dropout(y)\n",
    "        x = x + y\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "35f9a353",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, hidden_size, filter_size, dropout_rate, n_layers):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.embed = nn.Embedding(tokenizer_en.vocab_size + 2, 512, 40)\n",
    "        \n",
    "        encoders = [EncoderLayer(hidden_size, filter_size, dropout_rate)\n",
    "                    for _ in range(n_layers)]\n",
    "        self.layers = nn.ModuleList(encoders)\n",
    "\n",
    "        self.last_norm = nn.LayerNorm(hidden_size, eps=1e-6)\n",
    "        self.PE = PositionalEncoding(hidden_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        encoder_output = inputs\n",
    "        \n",
    "        encoder_output = self.embed(encoder_output)\n",
    "        \n",
    "        encoder_output = self.PE(encoder_output)\n",
    "        \n",
    "        for enc_layer in self.layers:\n",
    "            encoder_output = enc_layer(encoder_output)\n",
    "        return self.last_norm(encoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41dadd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_size, filter_size, dropout_rate):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.self_attention_norm = nn.LayerNorm(hidden_size, eps=1e-6)\n",
    "        self.self_attention = MultiHeadAttention(hidden_size, dropout_rate)\n",
    "        self.self_attention_dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.enc_dec_attention_norm = nn.LayerNorm(hidden_size, eps=1e-6)\n",
    "        self.enc_dec_attention = MultiHeadAttention(hidden_size, dropout_rate)\n",
    "        self.enc_dec_attention_dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.ffn_norm = nn.LayerNorm(hidden_size, eps=1e-6)\n",
    "        self.ffn = FeedForwardNetwork(hidden_size, filter_size, dropout_rate)\n",
    "        self.ffn_dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x, enc_output):\n",
    "        y = self.self_attention_norm(x)\n",
    "        y = self.self_attention(y, y, y)\n",
    "        y = self.self_attention_dropout(y)\n",
    "        x = x + y\n",
    "\n",
    "        if enc_output is not None:\n",
    "            y = self.enc_dec_attention_norm(x)\n",
    "            y = self.enc_dec_attention(y, enc_output, enc_output)\n",
    "            y = self.enc_dec_attention_dropout(y)\n",
    "            x = x + y\n",
    "\n",
    "        y = self.ffn_norm(x)\n",
    "        y = self.ffn(y)\n",
    "        y = self.ffn_dropout(y)\n",
    "        x = x + y\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "76808e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size, filter_size, dropout_rate, n_layers):\n",
    "        \n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.embed = nn.Embedding(tokenizer_en.vocab_size + 2, 512, 40)\n",
    "        \n",
    "        decoders = [DecoderLayer(hidden_size, filter_size, dropout_rate)\n",
    "                    for _ in range(n_layers)]\n",
    "        self.layers = nn.ModuleList(decoders)\n",
    "\n",
    "        self.last_norm = nn.LayerNorm(hidden_size, eps=1e-6)\n",
    "        self.PE = PositionalEncoding(hidden_size)\n",
    "\n",
    "    def forward(self, targets, enc_output):\n",
    "        \n",
    "        decoder_output = targets\n",
    "        \n",
    "        decoder_output = self.embed(decoder_output)\n",
    "        \n",
    "        decoder_output = self.PE(decoder_output)\n",
    "        \n",
    "        for i, dec_layer in enumerate(self.layers):\n",
    "            decoder_output = dec_layer(decoder_output, enc_output)\n",
    "\n",
    "        return self.last_norm(decoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "57f15ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, hidden_size, filter_size, dropout_rate):\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Linear(hidden_size, filter_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.layer2 = nn.Linear(filter_size, hidden_size)\n",
    "\n",
    "        initialize_weight(self.layer1)\n",
    "        initialize_weight(self.layer2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "6dd7c2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinalFeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, hidden_size, filter_size, dropout_rate):\n",
    "        super(FinalFeedForwardNetwork, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Linear(hidden_size, filter_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.layer2 = nn.Linear(filter_size, tokenizer_en.vocab_size + 2)\n",
    "\n",
    "        initialize_weight(self.layer1)\n",
    "        initialize_weight(self.layer2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "5a006070",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, max_len=40):\n",
    "        \"\"\"\n",
    "        Inputs\n",
    "            d_model - Hidden dimensionality of the input.\n",
    "            max_len - Maximum length of a sequence to expect.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Create matrix of [SeqLen, HiddenDim] representing the positional encoding for max_len inputs\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "\n",
    "        # register_buffer => Tensor which is not a parameter, but should be part of the modules state.\n",
    "        # Used for tensors that need to be on the same device as the module.\n",
    "        # persistent=False tells PyTorch to not add the buffer to the state dict (e.g. when we save the model)\n",
    "        self.register_buffer('pe', pe, persistent=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0c61e2",
   "metadata": {},
   "source": [
    "### Данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "0da76de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "params = {'EPOCHS': 2,\n",
    "          'DEVICE': 'cuda:0',\n",
    "          'BATCH': 64}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8f152e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ru_corpus_dev = torch.nn.utils.rnn.pad_sequence(ru_corpus, batch_first=True).to(params['DEVICE'])\n",
    "# en_corpus_dev = torch.nn.utils.rnn.pad_sequence(en_corpus, batch_first=True).to(params['DEVICE'])\n",
    "# en_target_dev = torch.nn.utils.rnn.pad_sequence(en_target, batch_first=True).to(params['DEVICE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "a41216e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu May  4 18:04:14 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 530.41.03              Driver Version: 530.41.03    CUDA Version: 12.1     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1060 6GB     Off| 00000000:07:00.0  On |                  N/A |\n",
      "|  0%   45C    P8                7W / 120W|    697MiB /  6144MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      1013      G   /usr/lib/xorg/Xorg                          284MiB |\n",
      "|    0   N/A  N/A      1350      G   /usr/bin/gnome-shell                         63MiB |\n",
      "|    0   N/A  N/A      2582      G   ...bian-installation/ubuntu12_32/steam       10MiB |\n",
      "|    0   N/A  N/A      2663      G   ...ion/logs/cef_log.txt --shared-files       43MiB |\n",
      "|    0   N/A  N/A      5126      G   ...sktop/4762/usr/bin/telegram-desktop        2MiB |\n",
      "|    0   N/A  N/A     16065      G   /usr/lib/firefox/firefox                    155MiB |\n",
      "|    0   N/A  N/A     47814      G   gnome-control-center                          1MiB |\n",
      "|    0   N/A  N/A     53306      G   ...78398894,8216697192962563347,131072      127MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "id": "91ba455b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simple_transf(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size=512):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = Encoder(hidden_size=512, filter_size=2048, dropout_rate=0.2, n_layers=4)\n",
    "        \n",
    "        self.decoder = Decoder(hidden_size=512, filter_size=2048, dropout_rate=0.2, n_layers=4)\n",
    "        \n",
    "        self.ff = FinalFeedForwardNetwork(512, filter_size=tokenizer_en.vocab_size + 2, dropout_rate=0.2)\n",
    "        \n",
    "    def forward(self, input_enc, input_target):\n",
    "        \n",
    "        x = encoder(input_enc)\n",
    "        y = decoder(input_target, x)\n",
    "        final = ff(y)\n",
    "        \n",
    "        return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9a34d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(range(len(ru_corpus)), batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "bde9c7dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5893\n",
      "10 , train_loss: tensor(8.9666, grad_fn=<NllLossBackward>)\n",
      "20 , train_loss: tensor(8.9723, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[468], line 29\u001b[0m\n\u001b[1;32m     25\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(prediction\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m64\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m13\u001b[39m,\u001b[38;5;241m8219\u001b[39m), true_en\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m64\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m13\u001b[39m))\n\u001b[1;32m     27\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 29\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     33\u001b[0m counter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/tensor.py:221\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Tensor \u001b[38;5;129;01mand\u001b[39;00m has_torch_function(relevant_args):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    215\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    216\u001b[0m         relevant_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    219\u001b[0m         retain_graph\u001b[38;5;241m=\u001b[39mretain_graph,\n\u001b[1;32m    220\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph)\n\u001b[0;32m--> 221\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:130\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m--> 130\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Simple_transf()\n",
    "print(5893)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer=optimizer, step_size=50, gamma=0.1)\n",
    "    \n",
    "best_model_wts = model.state_dict()\n",
    "best_loss = 1000\n",
    "counter = 0\n",
    "first = True\n",
    "for i in range(2):\n",
    "\n",
    "    time.sleep(0.2)\n",
    "    model.train(True)\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for bat in train_dataloader:\n",
    "        \n",
    "        batch_ru = torch.nn.utils.rnn.pad_sequence(ru_corpus[batch], batch_first=True)\n",
    "        batch_en = torch.nn.utils.rnn.pad_sequence(en_corpus[batch], batch_first=True)\n",
    "        true_en = torch.nn.utils.rnn.pad_sequence(en_target[batch], batch_first=True)\n",
    "        \n",
    "        prediction = model(batch_ru, batch_en)\n",
    "        loss = loss_fn(prediction.view(64*13,8219), true_en.view(64*13))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        counter += 1\n",
    "#         print(counter)\n",
    "        if counter%10==0:\n",
    "            print(counter,', train_loss:',loss)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    sheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d265453",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f387a983",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848e03fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "337738be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b95ce1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41af777d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c964bc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
